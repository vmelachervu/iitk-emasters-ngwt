{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "fD6TvJs03LLm",
      "metadata": {
        "id": "fD6TvJs03LLm"
      },
      "source": [
        "# EE932: Programmming Assignment 3\n",
        "**Name**:                               </br>\n",
        "**Roll No.**:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b199bd11-af1e-431e-80d9-0a230bfe4fc9",
      "metadata": {
        "id": "b199bd11-af1e-431e-80d9-0a230bfe4fc9"
      },
      "source": [
        "# Cliff Walking \n",
        "\n",
        "Through this gridworld exercise we will compare Sarsa and Q-learning algorithms, highlighting the difference between them. Consider the Cliff World shown in the assignment. This is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is −1 on all transitions except those into the the region marked “The Cliff.” Stepping into this region incurs a reward of −100 and sends the agent instantly back to the start. There are totally 48 states numbered from 0 to 47\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install pygame\n",
        "#!pip install \"gymnasium[toy-text]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mxfOaajLebI6",
      "metadata": {
        "id": "mxfOaajLebI6"
      },
      "outputs": [],
      "source": [
        "# Import\n",
        "%matplotlib inline\n",
        "import gymnasium as gym\n",
        "import itertools\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from collections import defaultdict\n",
        "matplotlib.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aw2PLF1fkcG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aw2PLF1fkcG",
        "outputId": "25085a7f-0746-4baf-8790-67901f07cc6b"
      },
      "outputs": [],
      "source": [
        "# Create environment\n",
        "# change render mode to 'ansi' to run the algorithm fast\n",
        "env = gym.make('CliffWalking-v0', render_mode='ansi')\n",
        "env.reset()\n",
        "env.render()  \n",
        "time.sleep(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LJRzERAip4mY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJRzERAip4mY",
        "outputId": "9b366d52-07a4-4686-b974-be21a4df0655"
      },
      "outputs": [],
      "source": [
        "env.step(1)\n",
        "# state, action, terminated?, truncated?, info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yX0i-19BfpzM",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yX0i-19BfpzM",
        "outputId": "289d8fe9-84d3-487b-fe1a-d64a8facba17"
      },
      "outputs": [],
      "source": [
        "# Allowed Actions\n",
        "# 0 => UP\n",
        "# 1 => Right\n",
        "# 2 => Down\n",
        "# 3 => Left\n",
        "\n",
        "# Let us take a few actions and get familiar with the environment\n",
        "print('We started here')\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "# Let us take up first\n",
        "print('Let us take UP action')\n",
        "env.step(0)\n",
        "env.render()\n",
        "\n",
        "# Now lets take another UP\n",
        "print('Another UP action')\n",
        "env.step(0)\n",
        "env.render()\n",
        "\n",
        "# Now lets take two rights\n",
        "print('Two Right actions')\n",
        "env.step(1)\n",
        "env.step(1)\n",
        "env.render()\n",
        "\n",
        "# Now one left\n",
        "print('One Left action')\n",
        "env.step(3)\n",
        "env.render()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vZB7clUFYxFq",
      "metadata": {
        "id": "vZB7clUFYxFq"
      },
      "outputs": [],
      "source": [
        "# A helper function for plotting the comparision graphs between two or more algorithms. \n",
        "# You need not understand this code.\n",
        "# Just look at the next cell to understand how to use it.\n",
        "\n",
        "def plot_episode_reward(a, stats, smoothing_window=10, noshow=False):\n",
        "    # Plot the episode reward over time\n",
        "    fig2 = plt.figure(figsize=(7,3))\n",
        "    for stat in stats:\n",
        "        rewards_smoothed = pd.Series(stat).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "        plt.plot(rewards_smoothed)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
        "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
        "    plt.legend(a)\n",
        "    if noshow:\n",
        "        plt.close(fig2)\n",
        "    else:\n",
        "        plt.show(fig2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oYM5MErceWe8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "id": "oYM5MErceWe8",
        "outputId": "c640fd78-54a9-42fb-cfcd-604a035deeea"
      },
      "outputs": [],
      "source": [
        "# Demo for using Plotting helper function\n",
        "# Here we are comparing two algorithms which were run for 3 episodes each. \n",
        "\n",
        "stats_algo1= [10,12,4]    #The return obtained in each episode for Algo1\n",
        "stats_algo2= [-1,12,90]    #The return obtained in each episode for Algo2\n",
        "\n",
        "#Plotting the comparision\n",
        "#Here the last argument is to smoothen the plot. Use higher values such as 10 for smooth plots.\n",
        "\n",
        "plot_episode_reward(['Algo1','Algo2'],[stats_algo1,stats_algo2],smoothing_window=1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04383be5-262d-4ba3-95b5-e64101737ced",
      "metadata": {
        "id": "04383be5-262d-4ba3-95b5-e64101737ced"
      },
      "source": [
        "### Epsilon-greedy policy from Q-function and epsilon\n",
        "\n",
        "Helper function to create epsilon-greedy policy based on a given Q-function and epsilon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f71f0b9-5cc0-457d-97e7-68c38e787eac",
      "metadata": {
        "id": "4f71f0b9-5cc0-457d-97e7-68c38e787eac"
      },
      "outputs": [],
      "source": [
        "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
        "    \"\"\"\n",
        "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
        "    \n",
        "    Args:\n",
        "        Q: A dictionary that maps from state -> action-values.\n",
        "            Each value is a numpy array of length nA (see below)\n",
        "        epsilon: The probability to select a random action. Float between 0 and 1.\n",
        "        nA: Number of actions in the environment.\n",
        "    \n",
        "    Returns:\n",
        "        A function that takes the state/observation as an argument and returns\n",
        "        the probabilities for each action in the form of a numpy array of length nA.\n",
        "    \n",
        "    \"\"\"\n",
        "    def policy_fn(observation):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        \n",
        "        if type(observation) is tuple:\n",
        "            best_action = np.argmax(Q[observation[0]])\n",
        "        else:\n",
        "            best_action = np.argmax(Q[observation])\n",
        "            \n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S1aEK0ULiMId",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1aEK0ULiMId",
        "outputId": "326212f6-62b7-4081-b75e-b262a1b7bba6"
      },
      "outputs": [],
      "source": [
        "# How to use make_epsilon-greedy_policy function\n",
        "# consider two states s1, s2 and 3 actions {0,1,2} in each state\n",
        "\n",
        "Q={'s1':[0.5, 2, 3], 's2':[-0.1, 4 , 3]}  # A sample Q-function Q(s,a) for each state-action pair\n",
        "\n",
        "# For this example it is clear that for state s1, action 2 is the greedy action.\n",
        "# For state s2, action 1 is the greedy action\n",
        "\n",
        "\"\"\"\n",
        "If we want an epsilon greedy policy with epsilon=0.3, then the best action\n",
        "should get a probility = 0.7 + 0.3/3 = 0.8\n",
        "other action with probability = 0.3/3 = 0.1\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Let us verify this using our make_epsilon-greedy_policy function\n",
        "\n",
        "policy_fn = make_epsilon_greedy_policy(Q, epsilon=0.3, nA=3)\n",
        "\n",
        "print(\"The epsilon-greedy policy at state s1 is \", policy_fn('s1'))\n",
        "print(\"The epsilon-greedy policy at state s2 is \", policy_fn('s2'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13048dac",
      "metadata": {},
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05ba0ef3-af67-45e0-959e-860ce2d15482",
      "metadata": {
        "id": "05ba0ef3-af67-45e0-959e-860ce2d15482"
      },
      "source": [
        "## Q1.1 SARSA algorithm\n",
        "\n",
        "Implement the SARSA algorithm using the following function template that retunts a tuple containing the optimal action value function and training statistics (array of cumulatetive rewards at each episode) **[3.5 Marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24cd60d-6411-40f1-8fdf-15cfcc13901a",
      "metadata": {
        "id": "f24cd60d-6411-40f1-8fdf-15cfcc13901a"
      },
      "outputs": [],
      "source": [
        "def sarsa(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    SARSA algorithm: Finds the optimal epsilon-greedy policy.\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI environment.\n",
        "        num_episodes: Number of episodes to run for.\n",
        "        discount_factor: Gamma discount factor.\n",
        "        alpha: TD learning rate.\n",
        "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
        "    \n",
        "    Returns:\n",
        "        A tuple (Q, stats).\n",
        "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
        "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
        "    \"\"\"\n",
        "    \n",
        "    # The final action-value function.\n",
        "    # A nested dictionary that maps state -> (action -> action-value).\n",
        "    Q = defaultdict(lambda: np.ones(env.action_space.n))\n",
        "    \n",
        "    # Keeps track of cumulative reward at each episode \n",
        "    stats = []\n",
        "    \n",
        "    for i_episode in range(num_episodes):\n",
        "        # Print out which episode we're on, useful for debugging.\n",
        "        if (i_episode + 1) % 10 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # The policy we're following\n",
        "        policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
        "    \n",
        "        # Reset the environment and pick the first action\n",
        "        state = env.reset()[0]\n",
        "        action_probs = policy(state)\n",
        "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            \n",
        "        total_reward=0\n",
        "        \n",
        "        # One step in the environment\n",
        "        for t in itertools.count():\n",
        "            # Generate the policy we're following from action value function\n",
        "            policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
        "            \n",
        "            # Select an action at the current state and take a step\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "            \n",
        "            # Pick the next action\n",
        "            next_action_probs = policy(next_state)\n",
        "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
        "            \n",
        "            # TD Update: SARSA \n",
        "            total_reward = 0       \n",
        "            \n",
        "            # break the loop if episode terminates\n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            # update the action and state variables\n",
        "            action = next_action\n",
        "            state = next_state\n",
        "        \n",
        "        # store the cumulative episode reward\n",
        "        stats.append(total_reward)\n",
        "    \n",
        "    return Q, stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KDY63bvCrgiY",
      "metadata": {
        "id": "KDY63bvCrgiY"
      },
      "outputs": [],
      "source": [
        "episodes = 200\n",
        "Q_sarsa, stats_sarsa = sarsa(env, episodes, epsilon=0.1)\n",
        "plot_episode_reward(['SARSA'],[stats_sarsa])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a543c1-d188-4253-874f-a687e454c476",
      "metadata": {
        "id": "c5a543c1-d188-4253-874f-a687e454c476"
      },
      "source": [
        "## Q1.2 Q-Learning algorithm\n",
        "\n",
        "Implement the Q-Learning algorithm using the following function template that retunts a tuple containing the optimal action value function and training statistics (array of cumulatetive rewards at each episode) **[3.5 Marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23dd2e16-5271-4c9b-86ab-6ce27e7ef0ea",
      "metadata": {
        "id": "23dd2e16-5271-4c9b-86ab-6ce27e7ef0ea"
      },
      "outputs": [],
      "source": [
        "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Q-Learning algorithm: Finds the optimal greedy policy\n",
        "    while following an epsilon-greedy policy\n",
        "    \n",
        "    Args:\n",
        "        env: OpenAI environment.\n",
        "        num_episodes: Number of episodes to run for.\n",
        "        discount_factor: Gamma discount factor.\n",
        "        alpha: TD learning rate.\n",
        "        epsilon: Chance to sample a random action. Float between 0 and 1.\n",
        "    \n",
        "    Returns:\n",
        "        A tuple (Q, episode_lengths).\n",
        "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
        "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
        "        target_rollout_stats is an EpisodeStats object with two numpy arrays for target_rollout_episode_lengths and target_rollout_episode_rewards.\n",
        "    \"\"\"\n",
        "    \n",
        "    # The final action-value function.\n",
        "    # Initialize a nested dictionary that maps state -> (action -> action-value).\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # Keeps track of cumulative reward at each episode \n",
        "    stats = []\n",
        "        \n",
        "    for i_episode in range(num_episodes):\n",
        "        # Print out which episode we're on, useful for debugging.\n",
        "        if (i_episode + 1) % 10 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "        \n",
        "        # The policy we're following\n",
        "        policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
        "        \n",
        "        # Reset the environment\n",
        "        state = env.reset()[0]        \n",
        "        \n",
        "        # One step in the environment\n",
        "        total_reward = 0\n",
        "        for t in itertools.count():\n",
        "            # The policy we're following\n",
        "            policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
        "            \n",
        "            # Take a step\n",
        "            action_probs = policy(state)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            # TD Update: Q-Learning\n",
        "            # write your code here\n",
        "\n",
        "            # break the loop if episode terminates\n",
        "            if done:\n",
        "                break \n",
        "            \n",
        "            # update the state\n",
        "            state = next_state\n",
        "        \n",
        "        # store the cumulative episode reward\n",
        "        stats.append(total_reward)\n",
        "    \n",
        "    return Q, stats"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c36fae6-f5dd-44eb-884b-3af17c2b3215",
      "metadata": {
        "id": "5c36fae6-f5dd-44eb-884b-3af17c2b3215"
      },
      "source": [
        "### Comparison and Generating Plots \n",
        "Plotting the reward over episodes for both SARSA and Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10c47078-9823-40b9-b6fc-6e5a3c7628fd",
      "metadata": {
        "id": "10c47078-9823-40b9-b6fc-6e5a3c7628fd"
      },
      "outputs": [],
      "source": [
        "episodes = 10\n",
        "Q_q, stats_q = q_learning(env, episodes, epsilon=0.2)\n",
        "Q_sarsa, stats_sarsa = sarsa(env, episodes, epsilon=0.2)\n",
        "plot_episode_reward(['Q-Learning Behavior Policy','SARSA'],[stats_q,stats_sarsa])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e438cda6",
      "metadata": {},
      "source": [
        "## Before we go on to visualize the policies generated by two TD methods answer the following questions:\n",
        "\n",
        "#### Q2. Why is Q-learning considered an off-policy control method? **[2 Marks]** <br>\n",
        "#### Q3. Which algorithm takes a more conservative path? **[1 Marks]** <br>\n",
        "#### Q4. **(Bonus)** Suppose while learning the action selection is greedy. Is Q-learning then exactly the same algorithm as Sarsa? Will they make exactly the same action selections and weight updates? **[2 Marks]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4bca118",
      "metadata": {},
      "source": [
        "### Answer:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27db3fb7-07b8-4e16-b8ba-74970b464504",
      "metadata": {
        "id": "27db3fb7-07b8-4e16-b8ba-74970b464504"
      },
      "source": [
        "### Visualizing the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ee3893",
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to visualize policy over the environment\n",
        "def render_policy(pi, mode='human', close=False):\n",
        "\n",
        "    cliff_pose = [(3,1),(3,2),(3,3),(3,4),(3,5),(3,6),(3,7),(3,8),(3,9),(3,10)] # cliff states\n",
        "    for s in range(48):\n",
        "        position = np.unravel_index(s, (4,12))\n",
        "        # print(self.s)\n",
        "        if pi[position] == 0:\n",
        "            output = \" ↑ \"\n",
        "        elif pi[position] == 1:\n",
        "            output = \" → \"\n",
        "        elif pi[position] == 2:\n",
        "            output = \" ↓ \"\n",
        "        elif pi[position] == 3:\n",
        "            output = \" ← \"\n",
        "\n",
        "        if position == (3,11):\n",
        "            output = \" T \"\n",
        "        elif position in cliff_pose:\n",
        "            output = \" G \"\n",
        "\n",
        "        if position[1] == 0:\n",
        "            output = output.lstrip()\n",
        "        if position[1] == 12 - 1:\n",
        "            output = output.rstrip()\n",
        "            output += \"\\n\"\n",
        "\n",
        "        print(output, end=\"\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f5a008b-4945-4663-9c3f-8296a7d8289e",
      "metadata": {
        "id": "7f5a008b-4945-4663-9c3f-8296a7d8289e"
      },
      "outputs": [],
      "source": [
        "# extracting greedy policy from the Q function\n",
        "pi_sarsa = np.zeros(env.observation_space.n)\n",
        "pi_q = np.zeros(env.observation_space.n)\n",
        "for s in range(env.observation_space.n):\n",
        "    pi_sarsa[s] = np.argmax(Q_sarsa[s]) \n",
        "    pi_q[s] = np.argmax(Q_q[s])\n",
        "    \n",
        "pi_sarsa = np.reshape(pi_sarsa, env.shape)\n",
        "pi_q = np.reshape(pi_q, env.shape)\n",
        "\n",
        "print('SARSA policy:')\n",
        "render_policy(pi_sarsa)\n",
        "print('Q-learning policy:')\n",
        "render_policy(pi_q)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
