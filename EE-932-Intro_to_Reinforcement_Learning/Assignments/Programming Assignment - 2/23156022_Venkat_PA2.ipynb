{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4e1c2-931b-4e68-9378-5d376b0df0ef",
   "metadata": {},
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597afc-35f8-4b03-8084-de771032604c",
   "metadata": {},
   "source": [
    "**Name:** Venkateswar Reddy Melachervu<br />\n",
    "**Roll No:23156022** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69751002-4656-47cf-8b2c-6016a434f4b6",
   "metadata": {},
   "source": [
    "## E1: A Deterministic Career Path\n",
    "\n",
    "Consider a simple Markov Decision Process below with four states and two actions available at each state. In this simplistic setting actions have deterministic effects, i.e., taking an action in a state always leads to one next state with transition probability equal to one. There are two actions out of each state for the agent to choose from: D for development and R for research. The _ultimately-care-only-about-money_ reward scheme is given along with the states.\n",
    "\n",
    "<img src='assets/mdp-d.png' width=\"700\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f991f3-9630-4656-9caa-135f13847ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afefbb-7b00-44e5-82dd-16953b59a7f3",
   "metadata": {},
   "source": [
    "### E1.1 Environment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61c01aa4-94c3-48e5-ad3b-279e03685260",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Career Path problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class CareerPathEnv(gym.Env):\n",
    "    '''\n",
    "    CareerPathEnv represents the Gym Environment for the Career Path problem environment\n",
    "    States : [0:'Unemployed', 1:'Industry', 2:'Grad School', 3:'Academia']\n",
    "    Actions : [0:'Research', 1:'Development']\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    state_names = {0:'Unemployed', 1:'Industry', 2:'Grad School', 3:'Academia'}\n",
    "    action_names = { 0:'Research', 1:'Development'}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=4,no_actions=2):\n",
    "        '''\n",
    "        Constructor for the CareerPath class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 4\n",
    "            no_actions : The no. of possible actions which is 2\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.prob_dynamics = {\n",
    "            # s: {\n",
    "            #   a: [(p(s,s'|a), s', r', terminal/not)]    \n",
    "            # }\n",
    "\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(1.0, 0, -10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        # print(\"Current state: {}\".format(self.state))\n",
    "        print(f\"Current state: {self.state} - {self.state_names.get(self.state)}\")\n",
    "\n",
    "    def render_action(self, action):\n",
    "        '''\n",
    "        Renders the action specified\n",
    "        '''\n",
    "        print(f\"Current action: {action} - {self.state_names.get(action)}\")\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "\n",
    "        # s: {\n",
    "        #   a: [(p(s,s'|a), s', r', terminal/not)]    \n",
    "        # }\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(1.0, 0, -10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][0]\n",
    "        self.state = dynamics_tuple[1]\n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9125c6e-8599-4dea-b388-b20596e33201",
   "metadata": {},
   "source": [
    "### E1.2 Policies\n",
    "\n",
    "After implementing the environment let us see how to make decisions in the environment. Let $\\pi_1(s) = R$ and $\\pi_2(s) = D$ for any state be two policies. Let us see how these policies look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d58aa48-25aa-4cb3-a70d-c4ac68f0cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research policy: \n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Development policy: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Random policy: \n",
      " [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "Uncertain policy: \n",
      " [[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "policy_R = np.concatenate((np.ones([4, 1]), np.zeros([4, 1])), axis=1)\n",
    "policy_D = np.concatenate((np.zeros([4, 1]), np.ones([4, 1])), axis=1)\n",
    "policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "print(\"Research policy: \\n\",policy_R)\n",
    "print(\"Development policy: \\n\", policy_D)\n",
    "print(\"Random policy: \\n\",policy_random)\n",
    "policy_uncertain = np.concatenate((0.5*np.ones([4, 1]), 0.5*np.ones([4, 1])), axis=1)\n",
    "print(\"Uncertain policy: \\n\",policy_uncertain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00cfd0",
   "metadata": {},
   "source": [
    "### E1.3 Testing\n",
    "\n",
    "By usine one of the above policies, lets see how we navigate the environment. We want to see how we take action based on a given policy, what state we transition to and obtain the rewards from the transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd4869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting the environment...\n",
      "Environment is reset.\n",
      "Current state: 1 - Industry\n",
      "Selected policy is: Development Policy\n",
      "State\t Action\t New State\t Reward\t is_Terminal\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t\t 100.0 \t   False\n",
      "Total Number of steps: 10\n",
      "Final Reward: 1000.0\n"
     ]
    }
   ],
   "source": [
    "# env = CareerPathEnv()\n",
    "env = CareerPathEnv(initial_state=1, no_states=4, no_actions=2)\n",
    "is_Terminal = False\n",
    "print('Resetting the environment...')\n",
    "start_state = env.reset()\n",
    "print('Environment is reset.')\n",
    "env.render()\n",
    "\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "# you may change policy here\n",
    "# policy = policy_uncertain\n",
    "policy = policy_D\n",
    "selected_policy = 'Development Policy'\n",
    "# policy = policy_D\n",
    "# policy = policy_random\n",
    "print('Selected policy is:', selected_policy)\n",
    "print(\"State\\t\", \"Action\\t\" , \"New State\\t\", \"Reward\\t\" , \"is_Terminal\")\n",
    "steps = 0\n",
    "max_steps = 5\n",
    "\n",
    "prev_state = start_state\n",
    "\n",
    "while steps < 10:\n",
    "    steps += 1\n",
    "\n",
    "    action = np.random.choice(2,1,p=policy[prev_state])[0]  #0 -> Research, 1 -> Development\n",
    "    state, reward, is_Terminal = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\" \",prev_state, \"\\t  \", action, \"\\t  \", state, \"\\t\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    prev_state = state\n",
    "    \n",
    "print(\"Total Number of steps:\", steps)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121e977-35c4-4845-888d-2b662262347a",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "Iterative Policy Evaluation is commonly used to calculate the state value function $V_\\pi(s)$ for a given policy $\\pi$. Here we implement a function to compute the state value function $V_\\pi(s)$ for a given policy\n",
    "\n",
    "<img src='assets/policy_eval.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c360b73b-e4d4-4538-b654-5837a123ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation\n",
    "def EvaluatePolicy(env, policy, gamma=0.9, theta=1e-8, draw=False):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            Vs = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P()[s][a]:\n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, np.abs(V[s]-Vs))\n",
    "            V[s] = Vs\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b891c3a",
   "metadata": {},
   "source": [
    "### Policy improvement\n",
    "\n",
    "$\\pi'(s) = \\arg \\max_a \\sum_{s',r} p(s',r|s,a)\\left[ r + \\gamma v_\\pi(s') \\right ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "930db58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Policy Improvement Function\n",
    "def ImprovePolicy(env, v, gamma):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    prob_dynamics = env.P()\n",
    "\n",
    "    q = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for state in prob_dynamics:\n",
    "            for action in prob_dynamics[state]:\n",
    "                #print(state, action)\n",
    "                for prob, new_state, reward, is_terminal in prob_dynamics[state][action]:\n",
    "                    #print(prob, new_state, reward, is_terminal)\n",
    "                    q[state][action] += prob*(reward + gamma*v[new_state])\n",
    "                        \n",
    "    new_pi = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for state in range(num_states):\n",
    "        opt_action = np.argmax(q[state])\n",
    "        new_pi[state][opt_action] = 1.0\n",
    "    \n",
    "    return new_pi       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8634e11",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "<img src='assets/policy_iteration.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1860233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy iteration function\n",
    "def PolicyIteration(env, pi, gamma, tol = 1e-10):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    iterations = 0\n",
    "    \n",
    "    while True:\n",
    "        # print(pi)\n",
    "        iterations += 1\n",
    "        pi_old = pi\n",
    "        v = EvaluatePolicy(env, pi_old, gamma, tol)\n",
    "        pi = ImprovePolicy(env, v, gamma)\n",
    "        \n",
    "        is_equal = True\n",
    "        for s in range(num_states):\n",
    "            if np.argmax(pi_old[s]) == np.argmax(pi[s]): \n",
    "                continue\n",
    "            is_equal = False\n",
    "        if is_equal == True:\n",
    "            break\n",
    "    return pi, v, iterations\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ce61",
   "metadata": {},
   "source": [
    "### Testing Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da9d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the environment for improving the policy using Policy Iteration...\n",
      "Initial Policy Details:\n",
      "State and Actions Values: \n",
      " [[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]]\n",
      "State and Actions Names:\n",
      "State 0 (Unemployed): Best Action -> 1 (Development)\n",
      "State 1 (Industry): Best Action -> 1 (Development)\n",
      "State 2 (Grad School): Best Action -> 0 (Research)\n",
      "State 3 (Academia): Best Action -> 1 (Development)\n",
      "\n",
      "Improving the policy using Policy Iteration...\n",
      "\n",
      "Improved/Iterated Policy Details:\n",
      "State and Actions Values: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "State and Actions Names:\n",
      "State 0 (Unemployed): Best Action -> 1 (Development)\n",
      "State 1 (Industry): Best Action -> 1 (Development)\n",
      "State 2 (Grad School): Best Action -> 1 (Development)\n",
      "State 3 (Academia): Best Action -> 1 (Development)\n",
      "State Values:  [1000. 1000. 1000. 1000.]\n",
      "Number of iterations:  2\n",
      "Iterations:\n",
      "Min\t Max\t Average\t Avg Iteration Time(S)\n",
      "1 \t 2 \t 1.97 \t\t 1.0573211660239903\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Test/iterate the chosen policy for improvement \n",
    "gamma = 0.9\n",
    "env = CareerPathEnv()\n",
    "chosen_policy_to_improve = policy_random\n",
    "print(\"Initializing the environment for improving the policy using Policy Iteration...\")\n",
    "print(\"Initial Policy Details:\")\n",
    "print(\"State and Actions Values: \\n\", chosen_policy_to_improve)\n",
    "# Render the policy in human readable form\n",
    "print(\"State and Actions Names:\")\n",
    "for s in range(env.nS):\n",
    "    print(f\"State {s} ({env.state_names[s]}): Best Action -> {np.argmax(chosen_policy_to_improve[s])} ({env.action_names[np.argmax(chosen_policy_to_improve[s])]})\")\n",
    "print()\n",
    "print(\"Improving the policy using Policy Iteration...\")\n",
    "print()\n",
    "pi, pi_v, pi_iters = PolicyIteration(env, chosen_policy_to_improve, gamma)\n",
    "print(\"Improved/Iterated Policy Details:\")\n",
    "print(\"State and Actions Values: \\n\", pi)\n",
    "# Render the policy in human readable form\n",
    "print(\"State and Actions Names:\")\n",
    "for s in range(env.nS):\n",
    "    print(f\"State {s} ({env.state_names[s]}): Best Action -> {np.argmax(pi[s])} ({env.action_names[np.argmax(pi[s])]})\")\n",
    "print(\"State Values: \", pi_v)\n",
    "print(\"Number of iterations: \",pi_iters)\n",
    "\n",
    "# average number of iterations required\n",
    "pi_avg_iters = 0\n",
    "pi_min_iters = 1000\n",
    "pi_max_iters = 0\n",
    "pi_start_time = time.time()\n",
    "r = 100\n",
    "for _ in range(r):\n",
    "    policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "    _, _, iters = PolicyIteration(env,policy_random, gamma)\n",
    "    pi_avg_iters += iters\n",
    "    pi_min_iters = min(pi_min_iters, iters)\n",
    "    pi_max_iters = max(pi_max_iters, iters)\n",
    "pi_end_time = time.time()\n",
    "pi_avg_iters /= 100\n",
    "pi_avg_iter_time = (pi_end_time - pi_start_time) / pi_avg_iters\n",
    "\n",
    "print(\"Iterations:\")\n",
    "print(\"Min\\t\", \"Max\\t\" , \"Average\\t\", \"Avg Iteration Time(S)\")\n",
    "print(pi_min_iters,\"\\t\", pi_max_iters,\"\\t\", pi_avg_iters, \"\\t\\t\", pi_avg_iter_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f1d6c-dd20-4902-9581-ce7f8a0ec944",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086d4b9",
   "metadata": {},
   "source": [
    "### A1. Find an optimal policy to navigate the given environment using Value Iteration (VI)\n",
    "\n",
    "<img src='assets/value_iteration.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82e66db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration function\n",
    "def value_iteration(env, gamma=0.9, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            V[s] = max(sum(p * (r + gamma * V[s_]) for p, s_, r, _ in env.prob_dynamics[s][a]) for a in range(env.nA))\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        q_values = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_values[a] = sum(p * (r + gamma * V[s_]) for p, s_, r, _ in env.prob_dynamics[s][a])\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s, best_action] = 1.0\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d88b5",
   "metadata": {},
   "source": [
    "### Testing Value Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90c71594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the environment for improving the policy using Value Iteration...\n",
      "\n",
      "Finding optimal policy using Value Iteration - Bellman optimality with update rule...\n",
      "\n",
      "Value Iterated Policy Details:\n",
      "State and Actions Values: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "State and Actions Names:\n",
      "State 0 (Unemployed): Best Action -> 1 (Development)\n",
      "State 1 (Industry): Best Action -> 1 (Development)\n",
      "State 2 (Grad School): Best Action -> 1 (Development)\n",
      "State 3 (Academia): Best Action -> 1 (Development)\n",
      "State Values:  [999.99999991 999.99999991 999.99999992 999.99999992]\n",
      "Number of Iterations:  100\n",
      "Avg Time Per Iteration (S): 0.0039990472793579104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "# Finding optimal policy using value iteration - turning Bellman optimality into update rule.\n",
    "\n",
    "# Initialize the environment\n",
    "print(\"Initializing the environment for improving the policy using Value Iteration...\")\n",
    "gamma = 0.9\n",
    "env = CareerPathEnv()\n",
    "\n",
    "# Perform value iteration\n",
    "print()\n",
    "print(\"Finding optimal policy using Value Iteration - Bellman optimality with update rule...\")\n",
    "\n",
    "vi_time_total = 0\n",
    "for _ in range(r): \n",
    "    vi_start_time = time.time()\n",
    "    vi, vi_state_values = value_iteration(env, gamma)\n",
    "    vi_end_time = time.time()\n",
    "    vi_time_elapsed = vi_end_time - vi_start_time\n",
    "    vi_time_total += vi_time_elapsed\n",
    "print()\n",
    "vi_avg_time = vi_time_total/r\n",
    "\n",
    "# Display the results\n",
    "print(\"Value Iterated Policy Details:\")\n",
    "print(\"State and Actions Values: \\n\", vi)\n",
    "# Render the policy in human readable form\n",
    "print(\"State and Actions Names:\")\n",
    "for s in range(env.nS):\n",
    "    print(f\"State {s} ({env.state_names[s]}): Best Action -> {np.argmax(vi[s])} ({env.action_names[np.argmax(vi[s])]})\")\n",
    "print(\"State Values: \", vi_state_values)\n",
    "print(\"Number of Iterations: \", r)\n",
    "print(\"Avg Time Per Iteration (S):\", vi_avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89de4dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of PI and VI in Deterministic Transition MDP\n",
      "Policy Iteration - Iterated Policy:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Policy Iteration - State Value Function:\n",
      "[1000.         1000.         1000.          990.10989011]\n",
      "Number of Iterations: 2\n",
      "\n",
      "Value Iteration - State and Action Values:\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Value Iteration State Value Function:\n",
      "[999.99999991 999.99999991 999.99999992 999.99999992]\n",
      "\n",
      "Comparison of States and Action:\n",
      "State 0 (Unemployed):\n",
      "  Value Iteration Best Action -> 1 (Development)\n",
      "  Policy Iteration Best Action -> 1 (Development)\n",
      "State 1 (Industry):\n",
      "  Value Iteration Best Action -> 1 (Development)\n",
      "  Policy Iteration Best Action -> 1 (Development)\n",
      "State 2 (Grad School):\n",
      "  Value Iteration Best Action -> 1 (Development)\n",
      "  Policy Iteration Best Action -> 1 (Development)\n",
      "State 3 (Academia):\n",
      "  Value Iteration Best Action -> 1 (Development)\n",
      "  Policy Iteration Best Action -> 1 (Development)\n",
      "Averare time taken per iteration by Policy Iteration (S): 1.0573211660239903\n",
      "Averare time taken per iteration by Value Iteration (S): 0.0039990472793579104\n"
     ]
    }
   ],
   "source": [
    "# Let's compare policy iteration and value iteration results and convergence\n",
    "print(\"Comparison of PI and VI in Deterministic Transition MDP\")\n",
    "print(\"Policy Iteration - Iterated Policy:\")\n",
    "pi, pi_v, pi_iters = PolicyIteration(env, chosen_policy_to_improve, gamma)\n",
    "print(pi)\n",
    "print(\"Policy Iteration - State Value Function:\")\n",
    "print(pi_v)\n",
    "print(\"Number of Iterations:\", pi_iters)\n",
    "print()\n",
    "\n",
    "print(\"Value Iteration - State and Action Values:\")\n",
    "print(vi)\n",
    "print(\"Value Iteration State Value Function:\")\n",
    "print(vi_state_values)\n",
    "\n",
    "# Render optimized policy in human readable form\n",
    "print()\n",
    "print(\"Comparison of States and Action:\")\n",
    "for s in range(env.nS):\n",
    "    print(f\"State {s} ({env.state_names[s]}):\")\n",
    "    print(f\"  Value Iteration Best Action -> {np.argmax(vi[s])} ({env.action_names[np.argmax(vi[s])]})\")\n",
    "    print(f\"  Policy Iteration Best Action -> {np.argmax(pi[s])} ({env.action_names[np.argmax(pi[s])]})\")\n",
    "\n",
    "print(\"Averare time taken per iteration by Policy Iteration (S):\", pi_avg_iter_time)\n",
    "print(\"Averare time taken per iteration by Value Iteration (S):\", vi_avg_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf1175",
   "metadata": {},
   "source": [
    "### A1.2 Compare PI and VI in terms of convergence (average number of iteration, time required for each iteration). Is the policy obtained by both same?\n",
    "- As can be seen above, the average time taken per iteration is **far less - 0.057 Seconds - for Value Iteration** vis-a-vis Policy Iteration  1.127 Seconds. \n",
    "\n",
    "- **The policy is deterministic in this case and hence Value Iteration does not need multiple iterations and will have convergence in one iteration.**\n",
    "\n",
    "- The optimal policy obtained for **both the cases is same** , as can be seen in the above convergence data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc712ac",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f6340",
   "metadata": {},
   "source": [
    "## Part B : A Stochastic Career Path\n",
    "\n",
    "Now consider a more realistic Markov Decision Process below with four states and two actions available at each state. In this setting Actions have nondeterministic effects, i.e., taking an action in a state always leads to one next state, but which state is the one next state is determined by transition probabilities. These transition probabilites are shown in the figure attached to the transition arrows from states and actions to states. There are two actions out of each state for the agent to choose from: D for development and R for research. The same _ultimately-care-only-about-money_ reward scheme is given along with the states.\n",
    "\n",
    "<img src='assets/mdp-nd.png' width=\"700\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a0ac3e-223e-42c9-896f-d3d74c060258",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Career Path problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticCareerPathEnv(gym.Env):\n",
    "    '''\n",
    "    StocasticCareerPathEnv represents the Gym Environment for the Career Path problem environment\n",
    "    States : [0:'Unemployed',1:'Industry',2:'Grad School',3:'Academia']\n",
    "    Actions : [0:'Research', 1:'Development']\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    state_names = {0:'Unemployed', 1:'Industry', 2:'Grad School', 3:'Academia'}\n",
    "    action_names = { 0:'Research', 1:'Development'}\n",
    "\n",
    "    def __init__(self,initial_state=3,no_states=4,no_actions=2):\n",
    "        '''\n",
    "        Constructor for the CareerPath class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 4\n",
    "            no_actions : The no. of possible actions which is 2\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.prob_dynamics = {\n",
    "            # s: {\n",
    "            #   a: [(p(s,s'|a), s', r', terminal/not), (p(s,s''|a), s'', r'', terminal/not)]    \n",
    "            # }\n",
    "            \n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.9, 0, -10.0, False),(0.1, 1, 100, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.9, 3, 10.0, False),(0.1, 2, 0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 1, 100, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 3, 10, False)],\n",
    "            },\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def render_action(self, action):\n",
    "        '''\n",
    "        Renders the action specified\n",
    "        '''\n",
    "        print(f\"Current action: {action} - {self.state_names.get(action)}\")\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.9, 0, -10.0, False),(0.1, 1, 100, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.9, 3, 10.0, False),(0.1, 2, 0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 1, 100, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 3, 10, False)],\n",
    "            },\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        if self.state == 0 or (self.state == 1 and action == 1) or (self.state == 3 and action == 0):\n",
    "            index = 0    \n",
    "        else:\n",
    "            index = np.random.choice(2,1,p=[0.9,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a5212",
   "metadata": {},
   "source": [
    "### Navigating in Stochastic Career Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e07dbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Career Path MDP and navigation \n",
    "class StochasticCareerPathEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    state_names = {0: 'Unemployed', 1: 'Industry', 2: 'Grad School', 3: 'Academia'}\n",
    "    action_names = {0: 'Research', 1: 'Development'}\n",
    "\n",
    "    def __init__(self, initial_state=0, no_states=4, no_actions=2):\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.prob_dynamics = {\n",
    "            0: {0: [(1.0, 2, 0.0, False)], 1: [(1.0, 1, 100.0, False)]},\n",
    "            1: {0: [(0.9, 0, -10.0, False), (0.1, 1, 100, False)], 1: [(1.0, 1, 100.0, False)]},\n",
    "            2: {0: [(0.9, 3, 10.0, False), (0.1, 2, 0, False)], 1: [(0.9, 1, 100.0, False), (0.1, 1, 100, False)]},\n",
    "            3: {0: [(1.0, 3, 10.0, False)], 1: [(0.9, 1, 100.0, False), (0.1, 3, 10, False)]},\n",
    "        }\n",
    "        # self.print_prob_dynamics()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        return self.state\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def render_action(self, action):\n",
    "        print(f\"Current action: {action} - {self.state_names.get(action)}\")\n",
    "\n",
    "    def sample_action(self):\n",
    "        return np.random.randint(0, self.nA)        \n",
    "\n",
    "    def P(self):\n",
    "        return self.prob_dynamics\n",
    "\n",
    "    def step(self, action):\n",
    "        if action >= self.nA:\n",
    "            action = self.nA - 1\n",
    "\n",
    "        if self.state == 0 or (self.state == 1 and action == 1) or (self.state == 3 and action == 0):\n",
    "            index = 0\n",
    "        else:\n",
    "            index = np.random.choice(2, 1, p=[0.9, 0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]\n",
    "    \n",
    "    def print_prob_dynamics(self):\n",
    "        for state, actions in self.prob_dynamics.items():\n",
    "            print(f\"State {state}:\")\n",
    "            for action, outcomes in actions.items():\n",
    "                print(f\"  Action {action}:\")\n",
    "                for prob, next_state, reward, terminal in outcomes:\n",
    "                    terminal_str = \"Terminal\" if terminal else \"Non-terminal\"\n",
    "                    print(f\"    -> (Prob: {prob}, Next State: {next_state}, Reward: {reward}, {terminal_str})\")\n",
    "\n",
    "def evaluate_policy(env, policy, gamma=0.9, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.prob_dynamics[s][a]:\n",
    "                    v += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            V[s] = v\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def improve_policy(env, V, gamma=0.9):\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        q_values = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_values[a] = sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, done in env.prob_dynamics[s][a])\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s, best_action] = 1.0\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, policy, gamma=0.9, theta=1e-8):\n",
    "    stable = False\n",
    "    iterations = 0\n",
    "    while not stable:\n",
    "        iterations += 1\n",
    "        V = evaluate_policy(env, policy, gamma, theta)\n",
    "        new_policy = improve_policy(env, V, gamma)\n",
    "        if np.all(policy == new_policy):\n",
    "            stable = True\n",
    "        policy = new_policy\n",
    "    return policy, V, iterations\n",
    "\n",
    "def value_iteration(env, gamma=0.9, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        iterations += 1\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            V[s] = max(sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, done in env.prob_dynamics[s][a]) for a in range(env.nA))\n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        q_values = np.zeros(env.nA)\n",
    "        for a in range(env.nA):\n",
    "            q_values[a] = sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, done in env.prob_dynamics[s][a])\n",
    "        best_action = np.argmax(q_values)\n",
    "        policy[s, best_action] = 1.0\n",
    "    return policy, V, iterations\n",
    "\n",
    "def measure_convergence(env, policy, method, gamma=0.9):\n",
    "    start_time = time.time()\n",
    "    if method == \"VI\":\n",
    "        policy, V, iterations = value_iteration(env, gamma)\n",
    "    elif method == \"PI\":\n",
    "        policy, V, iterations = policy_iteration(env, policy, gamma)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified.\")\n",
    "    end_time = time.time()\n",
    "    return policy, V, iterations, (end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8a7cdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing the environment for Stochastic Career Path Navigation...\n",
      "\n",
      "Initial Policy Details:\n",
      "State and Actions Values: \n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "State and Actions Names:\n",
      "State 0 (Unemployed): Best Action -> 0 (Research)\n",
      "State 1 (Industry): Best Action -> 0 (Research)\n",
      "State 2 (Grad School): Best Action -> 0 (Research)\n",
      "State 3 (Academia): Best Action -> 0 (Research)\n",
      "State Transition Probabilities/Stochasticity:\n",
      "State 0:\n",
      "  Action 0:\n",
      "    -> (Prob: 1.0, Next State: 2, Reward: 0.0, Non-terminal)\n",
      "  Action 1:\n",
      "    -> (Prob: 1.0, Next State: 1, Reward: 100.0, Non-terminal)\n",
      "State 1:\n",
      "  Action 0:\n",
      "    -> (Prob: 0.9, Next State: 0, Reward: -10.0, Non-terminal)\n",
      "    -> (Prob: 0.1, Next State: 1, Reward: 100, Non-terminal)\n",
      "  Action 1:\n",
      "    -> (Prob: 1.0, Next State: 1, Reward: 100.0, Non-terminal)\n",
      "State 2:\n",
      "  Action 0:\n",
      "    -> (Prob: 0.9, Next State: 3, Reward: 10.0, Non-terminal)\n",
      "    -> (Prob: 0.1, Next State: 2, Reward: 0, Non-terminal)\n",
      "  Action 1:\n",
      "    -> (Prob: 0.9, Next State: 1, Reward: 100.0, Non-terminal)\n",
      "    -> (Prob: 0.1, Next State: 1, Reward: 100, Non-terminal)\n",
      "State 3:\n",
      "  Action 0:\n",
      "    -> (Prob: 1.0, Next State: 3, Reward: 10.0, Non-terminal)\n",
      "  Action 1:\n",
      "    -> (Prob: 0.9, Next State: 1, Reward: 100.0, Non-terminal)\n",
      "    -> (Prob: 0.1, Next State: 3, Reward: 10, Non-terminal)\n",
      "\n",
      "Calculating total rewards for this policy with the provided stochastic state transitions for 10 steps...\n",
      "State\t Action\t New State\t Reward\t is_Terminal\n",
      "  0 \t   0 \t   2 \t\t 0.0 \t False\n",
      "  2 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "  3 \t   0 \t   3 \t\t 10.0 \t False\n",
      "Total Number of steps: 10\n",
      "Total Reward: 90.0\n"
     ]
    }
   ],
   "source": [
    "# Let's test SCP\n",
    "policy_R = np.concatenate((np.ones([4, 1]), np.zeros([4, 1])), axis=1)\n",
    "policy_D = np.concatenate((np.zeros([4, 1]), np.ones([4, 1])), axis=1)\n",
    "policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "\n",
    "# Initialize the environment\n",
    "gamma = 0.9\n",
    "env = StochasticCareerPathEnv()\n",
    "is_Terminal = False\n",
    "print(\"Initializing the environment for Stochastic Career Path Navigation...\")\n",
    "start_state = env.reset()\n",
    "print()\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "# you may change policy here\n",
    "chosen_policy_to_improve = policy_R\n",
    "# policy = policy_1\n",
    "# policy = policy_2\n",
    "print(\"Initial Policy Details:\")\n",
    "print(\"State and Actions Values: \\n\", chosen_policy_to_improve)\n",
    "# Render the policy in human readable form\n",
    "print(\"State and Actions Names:\")\n",
    "for s in range(env.nS):\n",
    "    print(f\"State {s} ({env.state_names[s]}): Best Action -> {np.argmax(chosen_policy_to_improve[s])} ({env.action_names[np.argmax(chosen_policy_to_improve[s])]})\")\n",
    "print(\"State Transition Probabilities/Stochasticity:\")\n",
    "env.print_prob_dynamics()\n",
    "print()\n",
    "\n",
    "steps_to_take = 10\n",
    "print(\"Calculating total rewards for this policy with the provided stochastic state transitions for\", steps_to_take, \"steps...\")\n",
    "print(\"State\\t\", \"Action\\t\" , \"New State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "steps = 0\n",
    "max_steps = 5\n",
    "\n",
    "prev_state = start_state\n",
    "while steps < steps_to_take:\n",
    "    steps += 1\n",
    "\n",
    "    action = np.random.choice(2,1,p=chosen_policy_to_improve[prev_state])[0]  #0 -> Research, 1 -> Development\n",
    "    state, reward, is_Terminal = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\" \",prev_state, \"\\t  \", action, \"\\t  \", state, \"\\t\\t\", reward, \"\\t\", is_Terminal)\n",
    "    prev_state = state\n",
    "    \n",
    "print(\"Total Number of steps:\", steps_to_take)\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a3612",
   "metadata": {},
   "source": [
    "### B1.1 Find an optimal policy to navigate the given SCP environment using Policy Iteration (PI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb1faa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of PI and VI in Stochastic Transition MDP\n",
      "Finding optimal policy using Policy Iteration...\n",
      "Policy Iteration: 2 iterations, 0.017524 seconds\n",
      "Iterated Policy Details:\n",
      "State and Actions Values: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "State and Actions Names:\n",
      "State 0 (Unemployed): Best Action -> 1 (Development)\n",
      "State 1 (Industry): Best Action -> 1 (Development)\n",
      "State 2 (Grad School): Best Action -> 1 (Development)\n",
      "State 3 (Academia): Best Action -> 1 (Development)\n",
      "State Values:  [999.99999991 999.99999991 999.99999992 990.10989003]\n",
      "Number of iterations:  2\n"
     ]
    }
   ],
   "source": [
    "# Policy Iteration for stochastic MDP\n",
    "print(\"Comparison of PI and VI in Stochastic Transition MDP\")\n",
    "print(\"Finding optimal policy using Policy Iteration...\")\n",
    "pi_policy, pi_V, pi_iterations, pi_time = measure_convergence(env, chosen_policy_to_improve, \"PI\")\n",
    "print(f\"Policy Iteration: {pi_iterations} iterations, {pi_time:.6f} seconds\")\n",
    "print(\"Iterated Policy Details:\")\n",
    "print(\"State and Actions Values: \\n\", pi_policy)\n",
    "# Render the policy in human readable form\n",
    "print(\"State and Actions Names:\")\n",
    "for s in range(env.nS):\n",
    "    print(f\"State {s} ({env.state_names[s]}): Best Action -> {np.argmax(pi_policy[s])} ({env.action_names[np.argmax(pi_policy[s])]})\")\n",
    "print(\"State Values: \", pi_V)\n",
    "print(\"Number of iterations: \", pi_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264b19c",
   "metadata": {},
   "source": [
    "### B1.2 Find an optimal policy to navigate the given SCP environment using Value Iteration (VI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77079cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal policy using Value Iteration...\n",
      "Value Iteration: 220 iterations, 0.008166 seconds\n",
      "Value Iterated Policy Details:\n",
      "State and Actions Values: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "State and Actions Names:\n",
      "State 0 (Unemployed): Best Action -> 1 (Development)\n",
      "State 1 (Industry): Best Action -> 1 (Development)\n",
      "State 2 (Grad School): Best Action -> 1 (Development)\n",
      "State 3 (Academia): Best Action -> 1 (Development)\n",
      "State Values:  [999.99999991 999.99999991 999.99999992 990.10989003]\n",
      "Number of iterations:  2\n"
     ]
    }
   ],
   "source": [
    "# Value Iteration for stochastic MDP\n",
    "# Measure convergence for Value Iteration\n",
    "print(\"Finding optimal policy using Value Iteration...\")\n",
    "vi_policy, vi_V, vi_iterations, vi_time = measure_convergence(env, \"\", \"VI\")\n",
    "print(f\"Value Iteration: {vi_iterations} iterations, {vi_time:.6f} seconds\")\n",
    "print(\"Value Iterated Policy Details:\")\n",
    "print(\"State and Actions Values: \\n\", pi_policy)\n",
    "# Render the policy in human readable form\n",
    "print(\"State and Actions Names:\")\n",
    "for s in range(env.nS):\n",
    "    print(f\"State {s} ({env.state_names[s]}): Best Action -> {np.argmax(pi_policy[s])} ({env.action_names[np.argmax(pi_policy[s])]})\")\n",
    "print(\"State Values: \", pi_V)\n",
    "print(\"Number of iterations: \", pi_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda774d8",
   "metadata": {},
   "source": [
    "### B1.3  Compare PI and VI in terms of convergence (average number of iteration, time required for each iteration). Is the policy obtained by both same for SCP environment?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c6f9de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policies obtained by VI and PI are the same: True\n"
     ]
    }
   ],
   "source": [
    "# write your code for comparison here\n",
    "policy_same = np.array_equal(vi_policy, pi_policy)\n",
    "print(f\"Policies obtained by VI and PI are the same: {policy_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61e7a8",
   "metadata": {},
   "source": [
    "The observations for convergence - time taken, number of iterations and obtained polcies comparison are detailed in the output statements in the above cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
