{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c4e1c2-931b-4e68-9378-5d376b0df0ef",
   "metadata": {},
   "source": [
    "# Programming Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d597afc-35f8-4b03-8084-de771032604c",
   "metadata": {},
   "source": [
    "**Name:** <br />\n",
    "**Roll No:** \n",
    "***\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- **Release Date**: **10th May 2024**  \n",
    "- **Deadline**: **22nd May 2024 11:59PM**\n",
    "- Kindly name your submission files as `RollNo_Name_PA2.ipynb`.  <br />\n",
    "- You are required to work out your answers and submit only the iPython Notebook. The code should be well commented and easy to understand as there are marks for this. This notebook can be used as a template for assignment submission. <br />\n",
    "- Submissions are to be made through iPearl portal. Submissions made through mail will not be graded.<br />\n",
    "- Answers to the theory questions if any should be included in the notebook itself. While using special symbols use the $\\LaTeX$ mode <br />\n",
    "- Make sure your plots are clear and have title, legends and clear lines, etc. <br />\n",
    "- Plagiarism of any form will not be tolerated. If your solutions are found to match with other students or from other uncited sources, there will be heavy penalties and the incident will be reported to the disciplinary authorities. <br />\n",
    "- In case you have any doubts, feel free to reach out to TAs for help. <br />\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69751002-4656-47cf-8b2c-6016a434f4b6",
   "metadata": {},
   "source": [
    "## E1: A Deterministic Career Path\n",
    "\n",
    "Consider a simple Markov Decision Process below with four states and two actions available at each state. In this simplistic setting actions have deterministic effects, i.e., taking an action in a state always leads to one next state with transition probability equal to one. There are two actions out of each state for the agent to choose from: D for development and R for research. The _ultimately-care-only-about-money_ reward scheme is given along with the states.\n",
    "\n",
    "<img src='assets/mdp-d.png' width=\"700\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f991f3-9630-4656-9caa-135f13847ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afefbb-7b00-44e5-82dd-16953b59a7f3",
   "metadata": {},
   "source": [
    "### E1.1 Environment Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c01aa4-94c3-48e5-ad3b-279e03685260",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Career Path problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class CareerPathEnv(gym.Env):\n",
    "    '''\n",
    "    CareerPathEnv represents the Gym Environment for the Career Path problem environment\n",
    "    States : [0:'Unemployed',1:'Industry',2:'Grad School',3:'Academia']\n",
    "    Actions : [0:'Research', 1:'Development']\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=0,no_states=4,no_actions=2):\n",
    "        '''\n",
    "        Constructor for the CareerPath class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 4\n",
    "            no_actions : The no. of possible actions which is 2\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.prob_dynamics = {\n",
    "            # s: {\n",
    "            #   a: [(p(s,s'|a), s', r', terminal/not)]    \n",
    "            # }\n",
    "\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(1.0, 0, -10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(1.0, 0, -10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][0]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9125c6e-8599-4dea-b388-b20596e33201",
   "metadata": {},
   "source": [
    "### E1.2 Policies\n",
    "\n",
    "After implementing the environment let us see how to make decisions in the environment. Let $\\pi_1(s) = R$ and $\\pi_2(s) = D$ for any state be two policies. Let us see how these policies look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d58aa48-25aa-4cb3-a70d-c4ac68f0cacc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research policy: \n",
      " [[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "Development policy: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "Random policy: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Uncertain policy: \n",
      " [[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "policy_R = np.concatenate((np.ones([4, 1]), np.zeros([4, 1])), axis=1)\n",
    "policy_D = np.concatenate((np.zeros([4, 1]), np.ones([4, 1])), axis=1)\n",
    "policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "print(\"Research policy: \\n\",policy_R)\n",
    "print(\"Development policy: \\n\", policy_D)\n",
    "print(\"Random policy: \\n\",policy_random)\n",
    "\n",
    "policy_uncertain = np.concatenate((0.5*np.ones([4, 1]), 0.5*np.ones([4, 1])), axis=1)\n",
    "print(\"Uncertain policy: \\n\",policy_uncertain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00cfd0",
   "metadata": {},
   "source": [
    "### E1.3 Testing\n",
    "\n",
    "By usine one of the above policies, lets see how we navigate the environment. We want to see how we make take and action based on a given policy, what state we transition to and obtain the rewards from the transition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd4869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\t Action\t New State\t Reward\t is_Terminal\n",
      "  0 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   0 \t   0 \t -10.0 \t   False\n",
      "  0 \t   0 \t   2 \t 0.0 \t   False\n",
      "  2 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   0 \t   3 \t 10.0 \t   False\n",
      "  3 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   0 \t   0 \t -10.0 \t   False\n",
      "  0 \t   0 \t   2 \t 0.0 \t   False\n",
      "  2 \t   0 \t   3 \t 10.0 \t   False\n",
      "Total Number of steps: 10\n",
      "Final Reward: 310.0\n"
     ]
    }
   ],
   "source": [
    "env = CareerPathEnv()\n",
    "is_Terminal = False\n",
    "start_state = env.reset()\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "# you may change policy here\n",
    "policy = policy_uncertain\n",
    "# policy = policy_R\n",
    "# policy = policy_D\n",
    "# policy = policy_random\n",
    "\n",
    "print(\"State\\t\", \"Action\\t\" , \"New State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "steps = 0\n",
    "max_steps = 5\n",
    "\n",
    "prev_state = start_state\n",
    "\n",
    "while steps < 10:\n",
    "    steps += 1\n",
    "\n",
    "    action = np.random.choice(2,1,p=policy[prev_state])[0]  #0 -> Research, 1 -> Development\n",
    "    state, reward, is_Terminal = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\" \",prev_state, \"\\t  \", action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    prev_state = state\n",
    "    \n",
    "print(\"Total Number of steps:\", steps)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121e977-35c4-4845-888d-2b662262347a",
   "metadata": {},
   "source": [
    "### Iterative Policy Evaluation\n",
    "Iterative Policy Evaluation is commonly used to calculate the state value function $V_\\pi(s)$ for a given policy $\\pi$. Here we implement a function to compute the state value function $V_\\pi(s)$ for a given policy\n",
    "\n",
    "<img src='assets/policy_eval.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c360b73b-e4d4-4538-b654-5837a123ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Evaluation\n",
    "def EvaluatePolicy(env, policy, gamma=0.9, theta=1e-8, draw=False):\n",
    "    V = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            Vs = 0\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P()[s][a]:\n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, np.abs(V[s]-Vs))\n",
    "            V[s] = Vs\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b891c3a",
   "metadata": {},
   "source": [
    "### Policy improvement\n",
    "\n",
    "$\\pi'(s) = \\arg \\max_a \\sum_{s',r} p(s',r|s,a)\\left[ r + \\gamma v_\\pi(s') \\right ]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "930db58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Policy Improvement Function\n",
    "def ImprovePolicy(env, v, gamma):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    prob_dynamics = env.P()\n",
    "\n",
    "    q = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for state in prob_dynamics:\n",
    "            for action in prob_dynamics[state]:\n",
    "                #print(state, action)\n",
    "                for prob, new_state, reward, is_terminal in prob_dynamics[state][action]:\n",
    "                    #print(prob, new_state, reward, is_terminal)\n",
    "                    q[state][action] += prob*(reward + gamma*v[new_state])\n",
    "                        \n",
    "    new_pi = np.zeros((num_states, num_actions))\n",
    "    \n",
    "    for state in range(num_states):\n",
    "        opt_action = np.argmax(q[state])\n",
    "        new_pi[state][opt_action] = 1.0\n",
    "    \n",
    "    return new_pi       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8634e11",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "\n",
    "<img src='assets/policy_iteration.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1860233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PolicyIteration(env, pi, gamma, tol = 1e-10):\n",
    "    num_states = env.nS\n",
    "    num_actions = env.nA\n",
    "    iterations = 0\n",
    "    \n",
    "    while True:\n",
    "        # print(pi)\n",
    "        iterations += 1\n",
    "        pi_old = pi\n",
    "        v = EvaluatePolicy(env, pi_old, gamma, tol)\n",
    "        pi = ImprovePolicy(env, v, gamma)\n",
    "        \n",
    "        is_equal = True\n",
    "        for s in range(num_states):\n",
    "            if np.argmax(pi_old[s]) == np.argmax(pi[s]): \n",
    "                continue\n",
    "            is_equal = False\n",
    "        if is_equal == True:\n",
    "            break\n",
    "    return pi, v, iterations\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5049ce61",
   "metadata": {},
   "source": [
    "### Testing Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9da9d38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Policy: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "Final Policy: \n",
      " [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "State Value Function:  [1000. 1000. 1000. 1000.]\n",
      "Number of iterations for Policy Iteration:  2\n",
      "Iterations:\n",
      "Min\t Max\t Average\n",
      "1 \t 2 \t 1.89\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "env = CareerPathEnv()\n",
    "\n",
    "print(\"Initial Policy: \\n\",policy_random)\n",
    "pi, v, iters = PolicyIteration(env, policy_random, gamma)\n",
    "print(\"Final Policy: \\n\",pi)\n",
    "print(\"State Value Function: \",v)\n",
    "print(\"Number of iterations for Policy Iteration: \",iters)\n",
    "\n",
    "# average number of iterations required\n",
    "avg_iters = 0\n",
    "min_iters = 1000\n",
    "max_iters = 0\n",
    "for _ in range(100):\n",
    "    policy_random = np.array((np.random.permutation(2), np.random.permutation(2), np.random.permutation(2), np.random.permutation(2)))\n",
    "    _, _, iters = PolicyIteration(env,policy_random, gamma)\n",
    "    avg_iters += iters\n",
    "    min_iters = min(min_iters, iters)\n",
    "    max_iters = max(max_iters, iters)\n",
    "avg_iters /= 100\n",
    "print(\"Iterations:\")\n",
    "print(\"Min\\t\", \"Max\\t\" , \"Average\")\n",
    "print(min_iters,\"\\t\", max_iters,\"\\t\", avg_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f1d6c-dd20-4902-9581-ce7f8a0ec944",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c086d4b9",
   "metadata": {},
   "source": [
    "### A1. Find an optimal policy to navigate the given environment using Value Iteration (VI)\n",
    "\n",
    "<img src='assets/value_iteration.png' width=\"500\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e66db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d88b5",
   "metadata": {},
   "source": [
    "### Testing Value Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90c71594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code for testing value iteration here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bf1175",
   "metadata": {},
   "source": [
    "### A1.2 Compare PI and VI in terms of convergence (average number of iteration, time required for each iteration). Is the policy obtained by both same?\n",
    "\n",
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc712ac",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47f6340",
   "metadata": {},
   "source": [
    "## Part B : A Stochastic Career Path\n",
    "\n",
    "Now consider a more realistic Markov Decision Process below with four states and two actions available at each state. In this setting Actions have nondeterministic effects, i.e., taking an action in a state always leads to one next state, but which state is the one next state is determined by transition probabilities. These transition probabilites are shown in the figure attached to the transition arrows from states and actions to states. There are two actions out of each state for the agent to choose from: D for development and R for research. The same _ultimately-care-only-about-money_ reward scheme is given along with the states.\n",
    "\n",
    "<img src='assets/mdp-nd.png' width=\"700\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a0ac3e-223e-42c9-896f-d3d74c060258",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Represents a Career Path problem Gym Environment which provides a Fully observable\n",
    "MDP\n",
    "'''\n",
    "class StochasticCareerPathEnv(gym.Env):\n",
    "    '''\n",
    "    StocasticCareerPathEnv represents the Gym Environment for the Career Path problem environment\n",
    "    States : [0:'Unemployed',1:'Industry',2:'Grad School',3:'Academia']\n",
    "    Actions : [0:'Research', 1:'Development']\n",
    "    '''\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self,initial_state=3,no_states=4,no_actions=2):\n",
    "        '''\n",
    "        Constructor for the CareerPath class\n",
    "\n",
    "        Args:\n",
    "            initial_state : starting state of the agent\n",
    "            no_states : The no. of possible states which is 4\n",
    "            no_actions : The no. of possible actions which is 2\n",
    "            \n",
    "        '''\n",
    "        self.initial_state = initial_state\n",
    "        self.state = self.initial_state\n",
    "        self.nA = no_actions\n",
    "        self.nS = no_states\n",
    "        self.prob_dynamics = {\n",
    "            # s: {\n",
    "            #   a: [(p(s,s'|a), s', r', terminal/not), (p(s,s''|a), s'', r'', terminal/not)]    \n",
    "            # }\n",
    "            \n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.9, 0, -10.0, False),(0.1, 1, 100, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.9, 3, 10.0, False),(0.1, 2, 0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 1, 100, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 3, 10, False)],\n",
    "            },\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the environment\n",
    "        Returns:\n",
    "            observations containing player's current state\n",
    "        '''\n",
    "        self.state = self.initial_state\n",
    "        return self.get_obs()\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        Returns the player's state as the observation of the environment\n",
    "        '''\n",
    "        return (self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        '''\n",
    "        Renders the environment\n",
    "        '''\n",
    "        print(\"Current state: {}\".format(self.state))\n",
    "\n",
    "    def sample_action(self):\n",
    "        '''\n",
    "        Samples and returns a random action from the action space\n",
    "        '''\n",
    "        return random.randint(0, self.nA)\n",
    "    def P(self):\n",
    "        '''\n",
    "        Defines and returns the probabilty transition matrix which is in the form of a nested dictionary\n",
    "        '''\n",
    "        self.prob_dynamics = {\n",
    "            0: {\n",
    "                0: [(1.0, 2, 0.0, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            1: {\n",
    "                0: [(0.9, 0, -10.0, False),(0.1, 1, 100, False)],\n",
    "                1: [(1.0, 1, 100.0, False)],\n",
    "            },\n",
    "            2: {\n",
    "                0: [(0.9, 3, 10.0, False),(0.1, 2, 0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 1, 100, False)],\n",
    "            },\n",
    "            3: {\n",
    "                0: [(1.0, 3, 10.0, False)],\n",
    "                1: [(0.9, 1, 100.0, False),(0.1, 3, 10, False)],\n",
    "            },\n",
    "        }\n",
    "        return self.prob_dynamics\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Performs the given action\n",
    "        Args:\n",
    "            action : action from the action_space to be taking in the environment\n",
    "        Returns:\n",
    "            observation - returns current state\n",
    "            reward - reward obtained after taking the given action\n",
    "            done - True if the episode is complete else False\n",
    "        '''\n",
    "        if action >= self.nA:\n",
    "            action = self.nA-1\n",
    "\n",
    "        if self.state == 0 or (self.state == 1 and action == 1) or (self.state == 3 and action == 0):\n",
    "            index = 0    \n",
    "        else:\n",
    "            index = np.random.choice(2,1,p=[0.9,0.1])[0]\n",
    "\n",
    "        dynamics_tuple = self.prob_dynamics[self.state][action][index]\n",
    "        self.state = dynamics_tuple[1]\n",
    "        \n",
    "\n",
    "        return self.state, dynamics_tuple[2], dynamics_tuple[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6a5212",
   "metadata": {},
   "source": [
    "### Navigating in Stochastic Career Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e69064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\t Action\t New State\t Reward\t is_Terminal\n",
      "  3 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "  1 \t   1 \t   1 \t 100.0 \t   False\n",
      "Total Number of steps: 10\n",
      "Final Reward: 1000.0\n"
     ]
    }
   ],
   "source": [
    "env = StochasticCareerPathEnv()\n",
    "is_Terminal = False\n",
    "start_state = env.reset()\n",
    "steps = 0\n",
    "total_reward = 0\n",
    "\n",
    "# you may change policy here\n",
    "policy = policy_random\n",
    "# policy = policy_1\n",
    "# policy = policy_2\n",
    "\n",
    "print(\"State\\t\", \"Action\\t\" , \"New State\\t\" , \"Reward\\t\" , \"is_Terminal\")\n",
    "steps = 0\n",
    "max_steps = 5\n",
    "\n",
    "prev_state = start_state\n",
    "\n",
    "while steps < 10:\n",
    "    steps += 1\n",
    "\n",
    "    action = np.random.choice(2,1,p=policy[prev_state])[0]  #0 -> Research, 1 -> Development\n",
    "    state, reward, is_Terminal = env.step(action)\n",
    "    \n",
    "    total_reward += reward\n",
    "\n",
    "    print(\" \",prev_state, \"\\t  \", action, \"\\t  \", state, \"\\t\", reward, \"\\t  \", is_Terminal)\n",
    "    prev_state = state\n",
    "    \n",
    "print(\"Total Number of steps:\", steps)\n",
    "print(\"Final Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a3612",
   "metadata": {},
   "source": [
    "### B1.1 Find an optimal policy to navigate the given SCP environment using Policy Iteration (PI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb1faa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Hint] What would change for the stochastic MDP in the Policy Iteration code from Part A?\n",
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3264b19c",
   "metadata": {},
   "source": [
    "### B1.2 Find an optimal policy to navigate the given SCP environment using Value Iteration (VI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77079cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Hint] What would change for the stochastic MDP in the Value Iteration code from Part A?\n",
    "# write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda774d8",
   "metadata": {},
   "source": [
    "### B1.3  Compare PI and VI in terms of convergence (average number of iteration, time required for each iteration). Is the policy obtained by both same for SCP environment?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c6f9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code for comparison here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a61e7a8",
   "metadata": {},
   "source": [
    "Write your comments compairing convergence and policies here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
